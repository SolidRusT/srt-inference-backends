{
  "name": "vllm-inference-api",
  "version": "1.1.0",
  "description": "API proxy for vLLM OpenAI-compatible inference service with comprehensive endpoint support",
  "main": "server.js",
  "scripts": {
    "start": "node server.js",
    "dev": "NODE_ENV=development node server.js"
  },
  "dependencies": {
    "express": "^4.18.2"
  },
  "engines": {
    "node": ">=14.0.0"
  }
}
