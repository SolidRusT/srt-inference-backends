[Unit]
Description=vLLM Inference Server
After=docker.service
Requires=docker.service

[Service]
Type=oneshot
RemainAfterExit=yes
ExecStartPre=-/usr/bin/docker stop vllm-server
ExecStartPre=-/usr/bin/docker rm vllm-server
ExecStart=/opt/inference/vllm/start-vllm.sh
ExecStop=/usr/bin/docker stop vllm-server
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target