#!/usr/bin/env python3
"""
Advanced vLLM API Test Script

This script provides comprehensive testing for the vLLM API,
supporting both text completion and chat completion.
"""

import argparse
import json
import requests
import time
import sys
from typing import Dict, List, Any, Optional

class VLLMClient:
    """Client for testing vLLM APIs."""
    
    def __init__(self, host: str, port: int, model: str = None):
        """Initialize the vLLM client.
        
        Args:
            host: The hostname of the vLLM server
            port: The port number of the vLLM server
            model: Optional model name to use
        """
        self.base_url = f"http://{host}:{port}/v1"
        self.model = model
        print(f"‚úì Configured client for server at {self.base_url}")
        if model:
            print(f"‚úì Using model: {model}")
        else:
            print("‚ÑπÔ∏è No model specified, server default will be used")
    
    def test_connection(self) -> bool:
        """Test the connection to the vLLM server."""
        try:
            # Try to get models - this is a lightweight API call to check connectivity
            response = requests.get(f"{self.base_url}/models", timeout=5)
            response.raise_for_status()
            print("‚úì Successfully connected to vLLM server")
            return True
        except requests.exceptions.RequestException as e:
            print(f"‚ùå Connection failed: {e}")
            return False
    
    def completion(self, prompt: str, max_tokens: int = 100, temperature: float = 0.7, 
                  stop: Optional[List[str]] = None) -> Dict[str, Any]:
        """Send a completion request to the vLLM server.
        
        Args:
            prompt: The prompt text
            max_tokens: Maximum number of tokens to generate
            temperature: Sampling temperature
            stop: List of stop sequences
            
        Returns:
            The response JSON
        """
        url = f"{self.base_url}/completions"
        
        data = {
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        if self.model:
            data["model"] = self.model
            
        if stop:
            data["stop"] = stop
        
        print(f"\nüîÑ Sending completion request to {url}")
        print(f"üìù Prompt: {prompt}")
        
        try:
            start_time = time.time()
            response = requests.post(
                url, 
                headers={"Content-Type": "application/json"}, 
                data=json.dumps(data),
                timeout=60
            )
            response.raise_for_status()
            end_time = time.time()
            
            result = response.json()
            print(f"‚úì Response received in {end_time - start_time:.2f} seconds")
            return result
        
        except requests.exceptions.RequestException as e:
            print(f"‚ùå Request failed: {e}")
            if hasattr(e, 'response') and e.response:
                try:
                    print(f"Response: {e.response.json()}")
                except:
                    print(f"Response text: {e.response.text}")
            return None
    
    def chat_completion(self, messages: List[Dict[str, str]], max_tokens: int = 100, 
                       temperature: float = 0.7, stream: bool = False) -> Dict[str, Any]:
        """Send a chat completion request to the vLLM server.
        
        Args:
            messages: List of message dictionaries with 'role' and 'content'
            max_tokens: Maximum number of tokens to generate
            temperature: Sampling temperature
            stream: Whether to stream the response
            
        Returns:
            The response JSON
        """
        url = f"{self.base_url}/chat/completions"
        
        data = {
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": stream
        }
        
        if self.model:
            data["model"] = self.model
        
        print(f"\nüîÑ Sending chat completion request to {url}")
        print(f"üìù Messages: {json.dumps(messages, indent=2)}")
        
        try:
            start_time = time.time()
            response = requests.post(
                url, 
                headers={"Content-Type": "application/json"}, 
                data=json.dumps(data),
                timeout=60,
                stream=stream
            )
            response.raise_for_status()
            
            if stream:
                print("\nüîÑ Streaming response:")
                for line in response.iter_lines():
                    if line:
                        try:
                            line_text = line.decode('utf-8')
                            if line_text.startswith('data: '):
                                line_json = json.loads(line_text[6:])
                                if line_json.get('choices') and len(line_json['choices']) > 0:
                                    delta = line_json['choices'][0].get('delta', {})
                                    if 'content' in delta and delta['content']:
                                        print(delta['content'], end='', flush=True)
                        except Exception as e:
                            print(f"\nError parsing stream: {e}")
                print("\n\n‚úì Stream completed")
                end_time = time.time()
                print(f"‚úì Response completed in {end_time - start_time:.2f} seconds")
                return {"status": "stream_completed"}
            else:
                end_time = time.time()
                result = response.json()
                print(f"‚úì Response received in {end_time - start_time:.2f} seconds")
                return result
        
        except requests.exceptions.RequestException as e:
            print(f"‚ùå Request failed: {e}")
            if hasattr(e, 'response') and e.response:
                try:
                    print(f"Response: {e.response.json()}")
                except:
                    print(f"Response text: {e.response.text}")
            return None

def display_response(response: Dict[str, Any]) -> None:
    """Display the response in a readable format."""
    if not response:
        return
    
    # For chat completion response
    if "choices" in response and len(response["choices"]) > 0:
        if "message" in response["choices"][0]:
            print("\nüì¨ Response message:")
            message = response["choices"][0]["message"]
            print(f"Role: {message.get('role', 'unknown')}")
            print(f"\nContent:\n{message.get('content', '')}")
        elif "text" in response["choices"][0]:
            print("\nüì¨ Generated text:")
            print(response["choices"][0]["text"])
    
    # Display usage information if available
    if "usage" in response:
        usage = response["usage"]
        print("\nüìä Token usage:")
        print(f"  Prompt tokens: {usage.get('prompt_tokens', 'N/A')}")
        print(f"  Completion tokens: {usage.get('completion_tokens', 'N/A')}")
        print(f"  Total tokens: {usage.get('total_tokens', 'N/A')}")

def main():
    parser = argparse.ArgumentParser(description="Test vLLM API")
    parser.add_argument("--host", default="localhost", help="vLLM server hostname")
    parser.add_argument("--port", type=int, default={{ vllm_port }}, help="vLLM server port")
    parser.add_argument("--model", default="{{ vllm_model }}", help="Model name (leave empty to use server default)")
    parser.add_argument("--interactive", "-i", action="store_true", help="Start in interactive mode")
    parser.add_argument("--chat", "-c", action="store_true", help="Test chat completion instead of text completion")
    parser.add_argument("--prompt", "-p", help="Prompt for completion (for non-interactive mode)")
    parser.add_argument("--max-tokens", type=int, default=100, help="Maximum tokens to generate")
    parser.add_argument("--temperature", type=float, default=0.7, help="Sampling temperature")
    
    args = parser.parse_args()
    
    client = VLLMClient(args.host, args.port, args.model)
    
    # Test the connection
    if not client.test_connection():
        print("‚ùå Could not connect to vLLM server. Please check that the server is running.")
        sys.exit(1)
    
    if args.chat:
        # Simple chat completion test
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": args.prompt or "Tell me a short joke."}
        ]
        
        response = client.chat_completion(messages, args.max_tokens, args.temperature)
        display_response(response)
    elif args.prompt:
        # Single prompt mode
        response = client.completion(args.prompt, args.max_tokens, args.temperature)
        display_response(response)
    else:
        # Default test with a simple prompt
        response = client.completion(
            "Write a short poem about artificial intelligence:",
            args.max_tokens,
            args.temperature
        )
        display_response(response)
    
    print("\nüëã Test completed.")

if __name__ == "__main__":
    main()
